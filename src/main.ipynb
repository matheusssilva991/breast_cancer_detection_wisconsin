{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecção de câncer de mama do banco de imagens Winsconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo - Prever se o câncer é benigno ou maligno.\n",
    "\n",
    "Conjunto de dados: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n",
       "0         0.2419  ...          17.33           184.60      2019.0   \n",
       "1         0.1812  ...          23.41           158.80      1956.0   \n",
       "2         0.2069  ...          25.53           152.50      1709.0   \n",
       "3         0.2597  ...          26.50            98.87       567.7   \n",
       "4         0.1809  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  Unnamed: 32  \n",
       "0          0.4601                  0.11890          NaN  \n",
       "1          0.2750                  0.08902          NaN  \n",
       "2          0.3613                  0.08758          NaN  \n",
       "3          0.6638                  0.17300          NaN  \n",
       "4          0.2364                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/data_cancer2.csv', index_col=0).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   diagnosis                569 non-null    object \n",
      " 1   radius_mean              569 non-null    float64\n",
      " 2   texture_mean             569 non-null    float64\n",
      " 3   perimeter_mean           569 non-null    float64\n",
      " 4   area_mean                569 non-null    float64\n",
      " 5   smoothness_mean          569 non-null    float64\n",
      " 6   compactness_mean         569 non-null    float64\n",
      " 7   concavity_mean           569 non-null    float64\n",
      " 8   concave points_mean      569 non-null    float64\n",
      " 9   symmetry_mean            569 non-null    float64\n",
      " 10  fractal_dimension_mean   569 non-null    float64\n",
      " 11  radius_se                569 non-null    float64\n",
      " 12  texture_se               569 non-null    float64\n",
      " 13  perimeter_se             569 non-null    float64\n",
      " 14  area_se                  569 non-null    float64\n",
      " 15  smoothness_se            569 non-null    float64\n",
      " 16  compactness_se           569 non-null    float64\n",
      " 17  concavity_se             569 non-null    float64\n",
      " 18  concave points_se        569 non-null    float64\n",
      " 19  symmetry_se              569 non-null    float64\n",
      " 20  fractal_dimension_se     569 non-null    float64\n",
      " 21  radius_worst             569 non-null    float64\n",
      " 22  texture_worst            569 non-null    float64\n",
      " 23  perimeter_worst          569 non-null    float64\n",
      " 24  area_worst               569 non-null    float64\n",
      " 25  smoothness_worst         569 non-null    float64\n",
      " 26  compactness_worst        569 non-null    float64\n",
      " 27  concavity_worst          569 non-null    float64\n",
      " 28  concave points_worst     569 non-null    float64\n",
      " 29  symmetry_worst           569 non-null    float64\n",
      " 30  fractal_dimension_worst  569 non-null    float64\n",
      " 31  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), object(1)\n",
      "memory usage: 142.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  fractal_dimension_mean  ...  texture_worst  \\\n",
       "count     569.000000              569.000000  ...     569.000000   \n",
       "mean        0.181162                0.062798  ...      25.677223   \n",
       "std         0.027414                0.007060  ...       6.146258   \n",
       "min         0.106000                0.049960  ...      12.020000   \n",
       "25%         0.161900                0.057700  ...      21.080000   \n",
       "50%         0.179200                0.061540  ...      25.410000   \n",
       "75%         0.195700                0.066120  ...      29.720000   \n",
       "max         0.304000                0.097440  ...      49.540000   \n",
       "\n",
       "       perimeter_worst   area_worst  smoothness_worst  compactness_worst  \\\n",
       "count       569.000000   569.000000        569.000000         569.000000   \n",
       "mean        107.261213   880.583128          0.132369           0.254265   \n",
       "std          33.602542   569.356993          0.022832           0.157336   \n",
       "min          50.410000   185.200000          0.071170           0.027290   \n",
       "25%          84.110000   515.300000          0.116600           0.147200   \n",
       "50%          97.660000   686.500000          0.131300           0.211900   \n",
       "75%         125.400000  1084.000000          0.146000           0.339100   \n",
       "max         251.200000  4254.000000          0.222600           1.058000   \n",
       "\n",
       "       concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "count       569.000000            569.000000      569.000000   \n",
       "mean          0.272188              0.114606        0.290076   \n",
       "std           0.208624              0.065732        0.061867   \n",
       "min           0.000000              0.000000        0.156500   \n",
       "25%           0.114500              0.064930        0.250400   \n",
       "50%           0.226700              0.099930        0.282200   \n",
       "75%           0.382900              0.161400        0.317900   \n",
       "max           1.252000              0.291000        0.663800   \n",
       "\n",
       "       fractal_dimension_worst  Unnamed: 32  \n",
       "count               569.000000          0.0  \n",
       "mean                  0.083946          NaN  \n",
       "std                   0.018061          NaN  \n",
       "min                   0.055040          NaN  \n",
       "25%                   0.071460          NaN  \n",
       "50%                   0.080040          NaN  \n",
       "75%                   0.092080          NaN  \n",
       "max                   0.207500          NaN  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trata valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis                    0\n",
       "radius_mean                  0\n",
       "texture_mean                 0\n",
       "perimeter_mean               0\n",
       "area_mean                    0\n",
       "smoothness_mean              0\n",
       "compactness_mean             0\n",
       "concavity_mean               0\n",
       "concave points_mean          0\n",
       "symmetry_mean                0\n",
       "fractal_dimension_mean       0\n",
       "radius_se                    0\n",
       "texture_se                   0\n",
       "perimeter_se                 0\n",
       "area_se                      0\n",
       "smoothness_se                0\n",
       "compactness_se               0\n",
       "concavity_se                 0\n",
       "concave points_se            0\n",
       "symmetry_se                  0\n",
       "fractal_dimension_se         0\n",
       "radius_worst                 0\n",
       "texture_worst                0\n",
       "perimeter_worst              0\n",
       "area_worst                   0\n",
       "smoothness_worst             0\n",
       "compactness_worst            0\n",
       "concavity_worst              0\n",
       "concave points_worst         0\n",
       "symmetry_worst               0\n",
       "fractal_dimension_worst      0\n",
       "Unnamed: 32                569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 32'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos a coluna Unnamed: 32 devido a todos os dados dela serem nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar em Features e Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=['diagnosis']), df['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trata dados categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.map({'M': 1, 'B': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos os dados Malignos para 1 e os Benignos para 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir dados em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos 70% dos dados para treino e 30% para teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancear os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis\n",
       "0    0.625628\n",
       "1    0.374372\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train before: (398, 30)\n",
      "Shape X_train_res after: (298, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "diagnosis\n",
       "0    0.5\n",
       "1    0.5\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_res, y_train_res = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "\n",
    "print(\"Shape X_train before:\", X_train.shape)\n",
    "print(\"Shape X_train_res after:\", X_train_res.shape)\n",
    "\n",
    "y_train_res.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratar escala dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "standart_scaler = StandardScaler()\n",
    "X_train_padr = standart_scaler.fit_transform(X_train_res)\n",
    "X_test_padr = standart_scaler.transform(X_test)\n",
    "\n",
    "standart_scaler = StandardScaler()\n",
    "X_padr = standart_scaler.fit_transform(X_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pca_variance(pca, title=\"PCA\"):\n",
    "    print(title)\n",
    "    print(\"Explained variance ratio:\", pca.explained_variance_ratio_[:5])\n",
    "    print(\"Sum of explained variance ratio:\", sum(pca.explained_variance_ratio_), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dados completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "Explained variance ratio: [0.98019972 0.01788607]\n",
      "Sum of explained variance ratio: 0.9980857994757741 \n",
      "\n",
      "PCA Padr\n",
      "Explained variance ratio: [0.43977449 0.19343507 0.09718649 0.06666227 0.04713911]\n",
      "Sum of explained variance ratio: 0.9838459577428579 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_res)\n",
    "\n",
    "pca_padr = PCA(n_components=14)\n",
    "X_padr_pca = pca_padr.fit_transform(X_padr)\n",
    "\n",
    "print_pca_variance(pca)\n",
    "print_pca_variance(pca_padr, \"PCA Padr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dados de treino/teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "Explained variance ratio: [0.97962995 0.01809356]\n",
      "Sum of explained variance ratio: 0.9977235163518947 \n",
      "\n",
      "PCA Padr\n",
      "Explained variance ratio: [0.43139626 0.20100885 0.10289088 0.06510546 0.04685948]\n",
      "Sum of explained variance ratio: 0.9844901344047925 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_res)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "pca_padr = PCA(n_components=14)\n",
    "X_train_padr_pca = pca_padr.fit_transform(X_train_padr)\n",
    "X_test_padr_pca = pca_padr.transform(X_test_padr)\n",
    "\n",
    "print_pca_variance(pca)\n",
    "print_pca_variance(pca_padr, \"PCA Padr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo das variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dados de Treino\n",
    "- X_train: dados originais\n",
    "- X_train_res: dados balanceados\n",
    "- X_train_padr: dados balanceados e padronizados\n",
    "- X_train_pca: dados balanceados com redução de dimensionalidade\n",
    "- X_train_padr_pca: dados balanceados, padronizados com redução de dimensionalidade\n",
    "\n",
    "2. Dados de teste\n",
    "- X_test: dados originais\n",
    "- X_test_padr: dados balanceados e padronizados\n",
    "- X_test_pca: dados balanceados com redução de dimensionalidade\n",
    "- X_test_padr_pca: dados balanceados, padronizados com redução de dimensionalidade\n",
    "\n",
    "3. Dados completos\n",
    "- X: dados originais\n",
    "- X_res: dados balanceados\n",
    "- X_padr: dados balanceados e padronizados\n",
    "- X_pca: dados balanceados com redução de dimensionalidade\n",
    "- X_padr_pca: dados balanceados, padronizados com redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecionar melhor processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Train/Valid\n",
      "Train Accuracy:  92.43697478991596\n",
      "Test Accuracy:  91.66666666666666 \n",
      "\n",
      "Naive Bayes - Cross Validation (Train)\n",
      "Mean:  91.95402298850574\n"
     ]
    }
   ],
   "source": [
    "X_train_2, X_valid, y_train_2, y_valid = train_test_split(X_train_padr, y_train_res,\n",
    "                                                          test_size=0.2, random_state=42)\n",
    "\n",
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(X_train_2, y_train_2)\n",
    "\n",
    "train_predict = naive_bayes.predict(X_train_2)\n",
    "valid_predict = naive_bayes.predict(X_valid)\n",
    "\n",
    "print(\"Naive Bayes - Train/Valid\")\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train_2, train_predict) * 100)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_valid, valid_predict) * 100, \"\\n\")\n",
    "\n",
    "naive_bayes = GaussianNB()\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_result = cross_val_score(naive_bayes, X_train_padr, y_train_res, cv=k_fold, scoring='accuracy')\n",
    "print(\"Naive Bayes - Cross Validation (Train)\")\n",
    "print(\"Mean: \", cv_result.mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Train/Test\n",
      "Train Accuracy:  92.61744966442953\n",
      "Test Accuracy:  92.98245614035088 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       108\n",
      "           1       0.92      0.89      0.90        63\n",
      "\n",
      "    accuracy                           0.93       171\n",
      "   macro avg       0.93      0.92      0.92       171\n",
      "weighted avg       0.93      0.93      0.93       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(X_train_padr, y_train_res)\n",
    "\n",
    "train_predict = naive_bayes.predict(X_train_padr)\n",
    "test_predict = naive_bayes.predict(X_test_padr)\n",
    "\n",
    "print(\"Naive Bayes - Train/Test\")\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train_res, train_predict) * 100)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_predict) * 100, \"\\n\")\n",
    "\n",
    "print(classification_report(y_test, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103,   5],\n",
       "       [  7,  56]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Cross Validation (Full)\n",
      "Mean:  91.74418604651164\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = GaussianNB()\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_result = cross_val_score(naive_bayes, X_padr, y_res, cv=k_fold, scoring='accuracy')\n",
    "print(\"Naive Bayes - Cross Validation (Full)\")\n",
    "print(\"Mean: \", cv_result.mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecionar melhor processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Original\n",
      "Train Accuracy:  92.85714285714286\n",
      "valid Accuracy:  91.66666666666666 \n",
      "\n",
      "Data: Padr\n",
      "Train Accuracy:  92.43697478991596\n",
      "valid Accuracy:  91.66666666666666 \n",
      "\n",
      "Data: PCA\n",
      "Train Accuracy:  86.1344537815126\n",
      "valid Accuracy:  88.33333333333333 \n",
      "\n",
      "Data: PCA Padr\n",
      "Train Accuracy:  88.65546218487394\n",
      "valid Accuracy:  91.66666666666666 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_datas = [X_train_res, X_train_padr, X_train_pca, X_train_padr_pca]\n",
    "names = [\"Original\", \"Padr\", \"PCA\", \"PCA Padr\"]\n",
    "\n",
    "for name, X_data in zip(names, X_datas):\n",
    "    X_train_2, X_valid, y_train_2, y_valid = train_test_split(X_data, y_train_res,\n",
    "                                                          test_size=0.2, random_state=42)\n",
    "    print(f\"Data: {name}\")\n",
    "    naive_bayes = GaussianNB()\n",
    "    naive_bayes.fit(X_train_2, y_train_2)\n",
    "\n",
    "    train_predict = naive_bayes.predict(X_train_2)\n",
    "    valid_predict = naive_bayes.predict(X_valid)\n",
    "\n",
    "    print(\"Train Accuracy: \", accuracy_score(y_train_2, train_predict) * 100)\n",
    "    print(\"valid Accuracy: \", accuracy_score(y_valid, valid_predict) * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliar dados treino/teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Train/Test\n",
      "Train Accuracy:  90.60402684563759\n",
      "Test Accuracy:  98.24561403508771 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       108\n",
      "           1       0.98      0.97      0.98        63\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.98      0.98       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(C=100, kernel='rbf')\n",
    "svm.fit(X_train_res, y_train_res)\n",
    "\n",
    "train_predict = svm.predict(X_train_res)\n",
    "test_predict = svm.predict(X_test)\n",
    "\n",
    "print(\"SVM - Train/Test\")\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train_res, train_predict) * 100)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_predict) * 100, \"\\n\")\n",
    "\n",
    "print(classification_report(y_test, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[107,   1],\n",
       "       [  2,  61]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Cross Validation (Full)\n",
      "Mean:  92.44186046511628\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(C=100, kernel='rbf')\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_result = cross_val_score(svm, X_res, y_res, cv=k_fold, scoring='accuracy')\n",
    "print(\"SVM - Cross Validation (Full)\")\n",
    "print(\"Mean: \", cv_result.mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecionar melhor processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Original\n",
      "Train Accuracy:  95.7983193277311\n",
      "valid Accuracy:  90.0 \n",
      "\n",
      "Data: Padr\n",
      "Train Accuracy:  98.73949579831933\n",
      "valid Accuracy:  93.33333333333333 \n",
      "\n",
      "Data: PCA\n",
      "Train Accuracy:  90.33613445378151\n",
      "valid Accuracy:  90.0 \n",
      "\n",
      "Data: PCA Padr\n",
      "Train Accuracy:  98.31932773109243\n",
      "valid Accuracy:  95.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_datas = [X_train_res, X_train_padr, X_train_pca, X_train_padr_pca]\n",
    "names = [\"Original\", \"Padr\", \"PCA\", \"PCA Padr\"]\n",
    "\n",
    "for name, X_data in zip(names, X_datas):\n",
    "    X_train_2, X_valid, y_train_2, y_valid = train_test_split(X_data, y_train_res,\n",
    "                                                          test_size=0.2, random_state=42)\n",
    "    print(f\"Data: {name}\")\n",
    "    logistic_regression = LogisticRegression(max_iter=10000)\n",
    "    logistic_regression.fit(X_train_2, y_train_2)\n",
    "\n",
    "    train_predict = logistic_regression.predict(X_train_2)\n",
    "    valid_predict = logistic_regression.predict(X_valid)\n",
    "\n",
    "    print(\"Train Accuracy: \", accuracy_score(y_train_2, train_predict) * 100)\n",
    "    print(\"valid Accuracy: \", accuracy_score(y_valid, valid_predict) * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecionar melhores Hyper parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 1, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best score:  97.30508474576271\n"
     ]
    }
   ],
   "source": [
    "lr_params = {'C': [0.1, 1, 10, 100, 1000],\n",
    "              'penalty': ['l2'],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
    "              'max_iter': [10000]}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), lr_params, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_padr_pca, y_train_res)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_ * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Train/Test\n",
      "Train Accuracy:  97.98657718120806\n",
      "Test Accuracy:  98.83040935672514 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       108\n",
      "           1       0.98      0.98      0.98        63\n",
      "\n",
      "    accuracy                           0.99       171\n",
      "   macro avg       0.99      0.99      0.99       171\n",
      "weighted avg       0.99      0.99      0.99       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(**grid_search.best_params_)\n",
    "logistic_regression.fit(X_train_padr_pca, y_train_res)\n",
    "\n",
    "train_predict = logistic_regression.predict(X_train_padr_pca)\n",
    "test_predict = logistic_regression.predict(X_test_padr_pca)\n",
    "\n",
    "print(\"Logistic Regression - Train/Test\")\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train_res, train_predict) * 100)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_predict) * 100, \"\\n\")\n",
    "\n",
    "print(classification_report(y_test, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[107,   1],\n",
       "       [  1,  62]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Cross Validation (Full)\n",
      "Mean:  96.22923588039868\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(**grid_search.best_params_)\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_result = cross_val_score(logistic_regression, X_padr_pca, y_res, cv=k_fold, scoring='accuracy')\n",
    "print(\"Logistic Regression - Cross Validation (Full)\")\n",
    "print(\"Mean: \", cv_result.mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecionar melhor processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Original\n",
      "Train Accuracy:  91.59663865546219\n",
      "valid Accuracy:  88.33333333333333 \n",
      "\n",
      "Data: Padr\n",
      "Train Accuracy:  97.89915966386555\n",
      "valid Accuracy:  95.0 \n",
      "\n",
      "Data: PCA\n",
      "Train Accuracy:  91.59663865546219\n",
      "valid Accuracy:  91.66666666666666 \n",
      "\n",
      "Data: PCA Padr\n",
      "Train Accuracy:  97.89915966386555\n",
      "valid Accuracy:  93.33333333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_datas = [X_train_res, X_train_padr, X_train_pca, X_train_padr_pca]\n",
    "names = [\"Original\", \"Padr\", \"PCA\", \"PCA Padr\"]\n",
    "\n",
    "for name, X_data in zip(names, X_datas):\n",
    "    X_train_2, X_valid, y_train_2, y_valid = train_test_split(X_data, y_train_res,\n",
    "                                                          test_size=0.2, random_state=42)\n",
    "    print(f\"Data: {name}\")\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train_2, y_train_2)\n",
    "\n",
    "    train_predict = knn.predict(X_train_2)\n",
    "    valid_predict = knn.predict(X_valid)\n",
    "\n",
    "    print(\"Train Accuracy: \", accuracy_score(y_train_2, train_predict) * 100)\n",
    "    print(\"valid Accuracy: \", accuracy_score(y_valid, valid_predict) * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecionar melhores hyper parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'metric': 'euclidean', 'n_neighbors': 15, 'weights': 'distance'}\n",
      "Best score:  94.954802259887\n"
     ]
    }
   ],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 15, 19],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_padr, y_train_res)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_ * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN - Train/Test\n",
      "Train Accuracy:  100.0\n",
      "Test Accuracy:  95.90643274853801 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       108\n",
      "           1       0.95      0.94      0.94        63\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.96      0.95      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(**grid_search.best_params_)\n",
    "knn.fit(X_train_padr, y_train_res)\n",
    "\n",
    "train_predict = knn.predict(X_train_padr)\n",
    "test_predict = knn.predict(X_test_padr)\n",
    "\n",
    "print(\"KNN - Train/Test\")\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train_res, train_predict) * 100)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_predict) * 100, \"\\n\")\n",
    "\n",
    "print(classification_report(y_test, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105,   3],\n",
       "       [  4,  59]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN - Cross Validation (Full)\n",
      "Mean:  95.04429678848282\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(**grid_search.best_params_)\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_result = cross_val_score(knn, X_padr_pca, y_res, cv=k_fold, scoring='accuracy')\n",
    "print(\"KNN - Cross Validation (Full)\")\n",
    "print(\"Mean: \", cv_result.mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecionar melhor processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datas = [X_train_res, X_train_padr, X_train_pca, X_train_padr_pca]\n",
    "names = [\"Original\", \"Padr\", \"PCA\", \"PCA Padr\"]\n",
    "\n",
    "for name, X_data in zip(names, X_datas):\n",
    "    X_train_2, X_valid, y_train_2, y_valid = train_test_split(X_data, y_train_res,\n",
    "                                                          test_size=0.2, random_state=42)\n",
    "    print(f\"Data: {name}\")\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train_2, y_train_2)\n",
    "\n",
    "    train_predict = knn.predict(X_train_2)\n",
    "    valid_predict = knn.predict(X_valid)\n",
    "\n",
    "    print(\"Train Accuracy: \", accuracy_score(y_train_2, train_predict) * 100)\n",
    "    print(\"valid Accuracy: \", accuracy_score(y_valid, valid_predict) * 100, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
